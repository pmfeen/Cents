precision: "16-mixed"
accelerator: auto
devices: auto
strategy: ddp_find_unused_parameters_false
gradient_accumulate_every: 4
log_every_n_steps: 1
batch_size: 512
max_epochs: 5000
base_lr: 1e-4
eval_after_training: False

checkpoint:
  save_last: True       # Save final model
  save_top_k: 3         # Save top 3 best models
  every_n_train_steps: null
  every_n_epochs: 20   # Save every 500 epochs

lr_scheduler_params:
  factor: 0.5
  patience: 200
  min_lr: 1.0e-5
  threshold: 1.0e-1
  threshold_mode: rel
  verbose: false
